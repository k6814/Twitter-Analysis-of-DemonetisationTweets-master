{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import necssary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data and look some 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data=pd.read_csv(\"Twitter_data_master_final.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Temp=[Data[\"date\"],Data[\"text\"]]\n",
    "Tweets_WithDate=pd.DataFrame(data=Temp).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79192, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets_WithDate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date    object\n",
       "text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets_WithDate.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tweets_WithDate[\"text\"]=Tweets_WithDate[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Gov_Ann = [\"Goverenment of India\", \"GOI\", \"Prime Ministerâ€™s Office\", \"PMOindia\", \"Prime Minister\", \"PM\", \"Narendra Modi\", \"Modi\", \"Reserve Bank of India\", \"RBI\", \"Mint Street\", \"Dr. Urjit Patel\", \"Urjit Patel\", \"Urjit\", \"Dr. Raghuram Rajan\", \"Raguram Rajan\", \"Rajan\", \"R3\", \"Parliament\", \"Parliamentary\", \"Parliamentary Committee\", \"Standing Committee on Finance\", \"Standing Committee\", \"Finance Ministry\", \"Ministry of Finance\", \"Finance Minister\", \"FM\", \"Arun Jaitley\", \"Jaitley\", \"Ministry of Commerce and Industry\", \"Ministry of Commerce\", \"Commerce Ministry\", \"Commerce Minister\", \"Nirmala Sitharaman\", \"Nirmala\", \"Niti Aayog\", \"Planning Commission\", \"Arvind Panagariya\", \"Bibek Debroy\", \"Chief Economic Adviser\", \"Economic Adviser\", \"Economic Advisor\", \"Arvind Subramanian\", \"Piyush Goyal\", \"All India Radio\", \"AIR\", \"Doordarshan\", \"Prasar Bharti\", \"Ministry of Information and Broadcasting\", \"Mann ki Baat\", \"Central Board of Direct Taxes\", \"CBDT\", \"Enforcement Directorate\", \"ED\", \"CBI\", \"FEMA\"]\n",
    "Reason1_FakeC = [\"surgical strike\", \"illegal activity\", \"action taken\", \"security issues\", \"security\", \"Pakistan\", \"Pakistani\", \"Islamabad\", \"Bangladesh\", \"terrorism\", \"Islamic terrorism\", \" religious terrorism\", \"terror\", \"terror trail\", \"terror finance\", \"Islamic fundamentalism\", \"Muslim fundamentalism\", \"religious fundamentalism\", \"border\", \"pilferage\", \"security features\", \"counterfeit\", \"fake\", \"illicit\", \"counterfeit money\", \"counter\"]\n",
    "Reason2_BlackM = [\"blackmoney\", \"black economy\", \"black wealth\", \"kaladhan\", \"shadow economy\", \"parallel economy\", \"informal economy\", \"concealing\", \"benami\", \"hoarding\", \" Income Declaration Scheme\", \"raids\", \"seizures\", \"seized\", \"undisclosed income\", \"money laundering\"]\n",
    "Reason3_CashL = [\"Cashless\", \"cash less\", \"less cash\", \"payment systems\", \"retail payments\", \"POS\", \"cards\", \"Pay TM\", \"mobile payment\", \"United Payment Interface\", \"UPI\", \"swipe machines\", \"e-payment\", \"internet banking\", \"cardless\", \"gateway\"]\n",
    "Pol_Parties = [\"Bhartiya Janata Party\", \"BJP\", \"Narendra Modi\", \"Indian National Congress\", \"congress\", \"INC\", \"BJP Spokesperson\", \"Congress Spokesperson\", \"Rahul Gandhi\", \"Manmohan Singh\", \"MMS\", \"Sashi Tharoor\", \"Randeep Surjewala\", \"Chidambaram\", \"Aam Aadmi Party\", \"AAP\", \"Arvind Kejriwal\", \"Kejriwal\", \"Mamata Banerjee\", \"Mamata\", \"Derek\", \"Nitish Kumar\", \"Nitish\", \"Mulayam\", \"Akhilesh\", \"Nitish\", \"Lalu Yadav\", \"Lalu\", \"Sitaram Yechuri\", \"Vijayan\", \"Election Commission\", \"Election Commissioner\", \"High Court\", \"Supreme Court\", \"Courts\", \"public interest litigation\", \"PIL\"]\n",
    "Media_Reactions = [\"Indian Express\", \"Financial Express\", \"Times of India\", \"ToI\", \"Economic Times\", \"ET\", \"Zee\", \"New Delhi Television\", \"NDTV\", \"Anandabazar\", \"ABP\", \"India Today\", \"The Hindu\", \"Hindustan Times\", \"HT\", \"Mint\", \"Sun\", \"rediff.com\", \"Firstpost\", \"Sun\", \"UTV\", \"CNBC\", \"Times Now\", \"Dainik Jagran\", \"Divya Bhaskar\", \"The Wire\", \"Economist\", \"Fianancial Times\", \"New York Times\", \"Forbes\", \"Wall Street\", \"BBC\", \"CNN\", \"Bloomberg\", \"CMIE\", \"IndiaSpend\", \"Indicus\", \"Rediff\", \"Newslaundry\", \"Quint\", \"Quora\", \"The Guardian\"]\n",
    "Support = [\"NAMO\", \"Modiji\", \"President\", \"Pranab Mukherjee\", \"Kailash Satyarthi\", \"crackdown\", \"long term\", \"banks\", \"bankers\", \"bank chairman\", \"Arundhuti\", \"Chanda Kochher\", \"Anand Mahindra\", \"Jindal\", \"Kunal Bahl\", \"mobile payment\", \"Pay TM\", \"Vijay Shankar\", \"Narayan Murthy\", \"Baba Ramdev\", \"Ramdev\", \"Sri Sri Ravi Shankar\", \"Sri Sri\", \"Cjhandrababu Naidu\", \"Pay TM\", \"GST\", \"Quraishi\", \"IMF\", \"Global Times\", \"Jyrki Katainen\", \"Justin Rowlatt\", \"Venezuala\", \"The Independent\", \"Lee Kuan\", \"Amitabh Baccchan\", \"Amitabh\", \"Aishwarya Rai\", \"J&K\", \"stone pelting\", \"hawala\", \"trafficking\", \"Naxalites\", \"Maoists\", \"E-commerce\", \"ecommerce\", \"raids\", \"seizures\", \"tax collection\"]\n",
    "Criticism = [\"Bhakt\", \"shortage\", \"long queues\", \"lengthy queue\", \"lengthy queues\", \"disruption\", \"slowdown\", \"muddled\", \"muzzled\", \"poorly planned\", \"poor planning\", \"poorly executed\", \"poor execution\", \"protest\", \"protests\", \"litigation\", \"strikes\", \"bandh\", \"dharna\", \"rally\", \"Akrosh Diwas\", \"crash\", \"crashed\", \"closed\", \"short term\", \"severe\", \"detrimental\", \"death\", \"deaths\", \"inconvenience\", \"rush\", \"growth\", \"GDP growth\", \"plunder\", \"loot\", \"bullion\", \"gold\", \"jewellery\", \"real estate\", \"86\", \"blood\", \"circulation\", \"six months\", \"leak\", \"leakage\", \"scam\", \"SBI\", \"Sanjeev Kamboj\", \"Akila\", \"Cabinet meeting\", \"signature\", \"tourism\", \"carpet bomgin\", \"Amartya Sen\", \"Kaushik Basu\", \"Subramaniam Swamy\", \"Arun Shourie\", \"Pronab Sen\", \"Prabhat Patnaik\", \"Ninan\", \"Deepak Parekh\", \"Steve Forbes\", \"Krugman\", \"Harvard Business Review\", \"Hitler\", \"shutdown\", \"loss\", \"manpower\", \"emergency\", \"Indira\", \"vegetables\", \"rot\", \"dump\", \"farmers\", \"farming\", \"small industries\", \"SME\", \"difficulties\", \"violence\", \"thrashing\", \"transportation\", \"truck\", \"toll\", \"rabi\", \"harvest\", \"unsold\", \"backdated entries\", \"drop\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting bag of words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lowerList(List):\n",
    "    lower = []\n",
    "    for i in List:\n",
    "        i = i.lower()\n",
    "        lower.append(i)\n",
    "    return lower\n",
    "Classes = [Gov_Ann, Reason1_FakeC, Reason2_BlackM, Reason3_CashL, Pol_Parties, Media_Reactions, Support, Criticism]\n",
    "Classes = [lowerList(Class) for Class in Classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing bag of words in dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Classes_Dict = {\n",
    "    'Gov_Ann': Classes[0],\n",
    "    'Reason1_FakeC': Classes[1],\n",
    "    'Reason2_BlackM': Classes[2],\n",
    "    'Reason3_CashL': Classes[3],\n",
    "    'Pol_Parties': Classes[4],\n",
    "    'Media_Reactions': Classes[5],\n",
    "    'Support': Classes[6],\n",
    "    'Criticism': Classes[7]\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) Calculating count of tweets belonging to each class (Stored in dictionary called count). \n",
    "\n",
    "2) Creating Dictionary (called Tweets) with Classes as keys and tweets as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "Tweets = {\n",
    "        'Gov_Ann': [],\n",
    "        'Reason1_FakeC': [],\n",
    "        'Reason2_BlackM': [],\n",
    "        'Reason3_CashL': [],\n",
    "        'Pol_Parties': [],\n",
    "        'Media_Reactions': [], \n",
    "        'Support': [],\n",
    "        'Criticism': []\n",
    "        }\n",
    "\n",
    "count = {\n",
    "        'Gov_Ann': 0,\n",
    "        'Reason1_FakeC': 0,\n",
    "        'Reason2_BlackM': 0,\n",
    "        'Reason3_CashL': 0,\n",
    "        'Pol_Parties': 0,\n",
    "        'Media_Reactions': 0,\n",
    "        'Support': 0,\n",
    "        'Criticism': 0\n",
    "        }\n",
    "\n",
    "def tweetsDict(tweets):\n",
    "    \n",
    "    for Class in Classes_Dict:\n",
    "        for word in Classes_Dict[Class]:\n",
    "            for tweet in enumerate(tweets['text']):\n",
    "                #clean_tweet = tweet[1].strip('.' '!' '?' ')' '(' '#' ',' ':' '-').lower()\n",
    "                #clean_tweet = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', clean_tweets, flags=re.MULTILINE)\n",
    "                #clean_tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+','',clean_tweet,\n",
    "                          #flags=re.MULTILINE)\n",
    "                #clean_tweet = re.sub(r'<[^>]+>','',clean_tweet,flags=re.MULTILINE)\n",
    "                #clean_tweet = re.sub(r\"http\\S+\", \"\", clean_tweet)\n",
    "                if word in tweet[1]:\n",
    "                    if [tweet[1], tweets['date'].loc[tweet[0]]] not in Tweets[Class]:\n",
    "                        count[Class] = count[Class] + 1\n",
    "                        Tweets[Class].append([tweet[1], tweets['date'].loc[tweet[0]]])\n",
    "                \n",
    "                        \n",
    "                        \n",
    "tweetsDict(Tweets_WithDate)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets not classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Unclassified(ClassDict, Tweets):\n",
    "    UnclassifiedT = Tweets.copy()\n",
    "    for Class in ClassDict:\n",
    "        for tweet in ClassDict[Class]:\n",
    "            UnclassifiedT.drop(UnclassifiedT[UnclassifiedT['text'] == tweet[0]].index, inplace = True)\n",
    "    return UnclassifiedT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tweets['Reason2_BlackM'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Total Number of Tweets Classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in count.values():\n",
    "    total = total+i\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrames for Class wise Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Criticism_Tweets = pd.DataFrame(Tweets['Criticism'])\n",
    "Gov_Ann_Tweets = pd.DataFrame(Tweets['Gov_Ann'])\n",
    "Media_Reactions_Tweets = pd.DataFrame(Tweets['Media_Reactions'])\n",
    "Pol_Parties_Tweets = pd.DataFrame(Tweets['Pol_Parties'])\n",
    "Reason1_FakeC_Tweets = pd.DataFrame(Tweets['Reason1_FakeC'])\n",
    "Reason2_BlackM_Tweets = pd.DataFrame(Tweets['Reason2_BlackM'])\n",
    "Reason3_CashL_Tweets = pd.DataFrame(Tweets['Reason3_CashL'])\n",
    "Support_Tweets = pd.DataFrame(Tweets['Support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DailyCount(df):\n",
    "    for i in range(len(df)):\n",
    "        df[1].loc[i] = df[1].loc[i][:10]\n",
    "    df = df.groupby([1])[0].count()\n",
    "    df = pd.DataFrame(df)\n",
    "    df['date'] = df.index\n",
    "    df.index = range(len(df))\n",
    "    df.columns = ['Number of Tweets', 'Date']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Criticism_Count = DailyCount(Criticism_Tweets)\n",
    "Gov_Ann_Count = DailyCount(Gov_Ann_Tweets)\n",
    "Media_Reactions_Count = DailyCount(Media_Reactions_Tweets)\n",
    "Pol_Parties_Count = DailyCount(Pol_Parties_Tweets)\n",
    "Reason1_FakeC_Count = DailyCount(Reason1_FakeC_Tweets)\n",
    "Reason2_BlackM_Count = DailyCount(Reason2_BlackM_Tweets)\n",
    "Reason3_CashL_Count = DailyCount(Reason3_CashL_Tweets)\n",
    "Support_Count = DailyCount(Support_Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(path='ClassWiseDailyCount.xlsx', engine='xlsxwriter', options={'strings_to_urls': False})\n",
    "Criticism_Count.to_excel(writer, sheet_name='Criticism')\n",
    "Gov_Ann_Count.to_excel(writer, sheet_name='Government Announcements')\n",
    "Media_Reactions_Count.to_excel(writer, sheet_name='Media Reactions')\n",
    "Pol_Parties_Count.to_excel(writer, sheet_name='Political Parties')\n",
    "Reason1_FakeC_Count.to_excel(writer, sheet_name='Fake Currency Reason')\n",
    "Reason2_BlackM_Count.to_excel(writer, sheet_name='Black Money Reason')\n",
    "Reason3_CashL_Count.to_excel(writer, sheet_name='Cashless Economy Reason')\n",
    "Support_Count.to_excel(writer, sheet_name=\"Support\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exporting the DataFrames to Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(path='C:/Users/Sachit/Desktop/BDAP/NLP Project/Demonetisation/ClassWiseTweets.xlsx', engine='xlsxwriter', options={'strings_to_urls': False})\n",
    "Criticism_Tweets.to_excel(writer, sheet_name='Criticism')\n",
    "Gov_Ann_Tweets.to_excel(writer, sheet_name='Government Announcements')\n",
    "Media_Reactions_Tweets.to_excel(writer, sheet_name='Media Reactions')\n",
    "Pol_Parties_Tweets.to_excel(writer, sheet_name='Political Parties')\n",
    "Reason1_FakeC_Tweets.to_excel(writer, sheet_name='Fake Currency Reason')\n",
    "Reason2_BlackM_Tweets.to_excel(writer, sheet_name='Black Money Reason')\n",
    "Reason3_CashL_Tweets.to_excel(writer, sheet_name='Cashless Economy Reason')\n",
    "Support_Tweets.to_excel(writer, sheet_name=\"Support\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "for tweet in Tweets['Criticism']:\n",
    "    words = re.findall(r\"[\\w']+\", tweet[0].lower())\n",
    "    if 'scam' in words:\n",
    "        a.append(tweet[0].count('scam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = []\n",
    "for tweet in Tweets_WithDate['text']:\n",
    "    wordl = re.findall(r\"[\\w']+\", tweet)\n",
    "    if 'scam' in wordl:\n",
    "        b.append(tweet.count('scam'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Class Wise Word Count of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "\n",
    "TotalWC = {}\n",
    "\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words.extend(['-', '&', 'â€¦', '', 'â€“', 'http://','goo.gl' ':', ',', ':'])\n",
    "    \n",
    "\n",
    "def classwordcount(tweets):\n",
    "   \n",
    "    output = {}\n",
    "    for key in tweets:\n",
    "        output[key] = {}\n",
    "        \n",
    "    for Class in tweets:\n",
    "        for tweet in tweets[Class]:\n",
    "            words = re.findall(r\"[\\w']+\", tweet[0].lower())\n",
    "            for word in words:\n",
    "                if word not in stop_words:\n",
    "                    if word not in output[Class]:\n",
    "                        output[Class][word] = 1\n",
    "                    else:\n",
    "                        output[Class][word] = output[Class][word] + 1\n",
    "                    if word not in TotalWC:\n",
    "                        TotalWC[word] = 1\n",
    "                    else:\n",
    "                        TotalWC[word] = TotalWC[word] + 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ClassWordCount = classwordcount(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ClassWordCount['Criticism']['scam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Tweets_WithDate['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "WC = {}\n",
    "for tweet in Tweets_WithDate['text']:\n",
    "    words = re.findall(r\"[\\w']+\", tweet.lower())\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            if word in WC:\n",
    "                WC[word] = WC[word] + 1\n",
    "            else:\n",
    "                WC[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining function to sort dictionary items based on values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WCD = sortDict(WC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WCD = pd.DataFrame(WCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(path='C:/Users/Sachit/Desktop/BDAP/NLP Project/Demonetisation/TotalWordCount.xlsx', engine='xlsxwriter', options={'strings_to_urls': False})\n",
    "WCD.to_excel(writer, sheet_name='Total Word Count')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in sortDict(WC):\n",
    "    if 'demonetisation' in i[0]:\n",
    "        d.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrames for the classwise wordcount calculated previously (Sorted in descending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Gov_Ann_WC_S = pd.DataFrame(data=sortDict(ClassWordCount['Gov_Ann']), columns=[\"Word\", 'Count'])\n",
    "Reason1_FakeC_WC_S = pd.DataFrame(data=sortDict(ClassWordCount['Reason1_FakeC']), columns=[\"Word\", 'Count'])\n",
    "Reason2_BlackM_WC_S = pd.DataFrame(data=sortDict(ClassWordCount['Reason2_BlackM']), columns=[\"Word\", 'Count'])\n",
    "Reason3_CashL_WC_S = pd.DataFrame(data=sortDict(ClassWordCount[\"Reason3_CashL\"]), columns=[\"Word\", 'Count'])\n",
    "Pol_Parties_WC_S = pd.DataFrame(data=sortDict(ClassWordCount[\"Pol_Parties\"]), columns=[\"Word\", 'Count'])\n",
    "Media_Reactions_WC_S = pd.DataFrame(data=sortDict(ClassWordCount[\"Media_Reactions\"]), columns=['Word', 'Count'])\n",
    "Support_WC_S = pd.DataFrame(data=sortDict( ClassWordCount[\"Support\"]), columns=[\"Word\", 'Count'])\n",
    "Criticism_WC_S = pd.DataFrame(data=sortDict(ClassWordCount['Criticism']), columns=[\"Word\", 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting WordCount DataFrames to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(path='C:/Users/Sachit/Desktop/BDAP/NLP Project/Demonetisation/ClassWiseWordCount.xlsx', engine='xlsxwriter')\n",
    "Criticism_WC_S.to_excel(writer, sheet_name='Criticism')\n",
    "Gov_Ann_WC_S.to_excel(writer, sheet_name='Government Announcements')\n",
    "Media_Reactions_WC_S.to_excel(writer, sheet_name='Media Reactions')\n",
    "Pol_Parties_WC_S.to_excel(writer, sheet_name='Political Parties')\n",
    "Reason1_FakeC_WC_S.to_excel(writer, sheet_name='Fake Currency Reason')\n",
    "Reason2_BlackM_WC_S.to_excel(writer, sheet_name='Black Money Reason')\n",
    "Reason3_CashL_WC_S.to_excel(writer, sheet_name='Cashless Economy Reason')\n",
    "Support_WC_S.to_excel(writer, sheet_name=\"Support\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = Criticism_WC_S.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictC = df.set_index('Word')['Count'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OccurenceRate = {}\n",
    "\n",
    "for word in dictC:\n",
    "    OccurenceRate[word] = dictC[word]/TotalWC[word]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortDict(OccurenceRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rate2 = {}\n",
    "for word in dictC:\n",
    "    Rate2[word] = dictC[word]/WC[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortDict(Rate2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WC['scam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictC['scam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "x=[\"November\",\"December\",\"January\",\"Feb\"]\n",
    "b=sns.barplot(Month_tweets.index,y=Month_tweets.values)\n",
    "b.set_title('No. of Tweets for each month')\n",
    "b.set_xlabel(\"month\")\n",
    "b.set_ylabel(\"No. of Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(count, key=count.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tweets_per_class=[]\n",
    "Classes1 = ['Reason2_BlackM', 'Reason1_FakeC', 'Media_Reactions', 'Reason3_CashL', 'Support', 'Pol_Parties', 'Criticism', 'Gov_Ann']\n",
    "for Class in Classes1:\n",
    "    Tweets_per_class.append(len(Tweets[Class]))\n",
    "\n",
    "Tweets_per_class\n",
    "len(Tweets['Reason2_BlackM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "Classes1 = ['Reason2_BlackM', 'Reason1_FakeC', 'Media_Reactions', 'Reason3_CashL', 'Support', 'Pol_Parties', 'Criticism', 'Gov_Ann']\n",
    "_ROTATION_DEGREES = 90\n",
    "b=sns.barplot(x=Classes1,y=np.sort(list(Tweets_per_class)))\n",
    "b.set_xticklabels(Classes1, rotation=_ROTATION_DEGREES)\n",
    "b.set_title('No. of Tweets for each class')\n",
    "b.set_xlabel(\"Classes\")\n",
    "b.set_ylabel(\"No. of Tweets\")\n",
    "b.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A=Tweets['Criticism'][0]\n",
    "#weets_WithDate[Data['text']==A]\n",
    "Tweets_WithDate['only_date'] = pd.DatetimeIndex(Tweets_WithDate['date']).date\n",
    "Tweets_WithDate.head()\n",
    "tweets_date=Tweets_WithDate.groupby(['only_date']).count()\n",
    "tweets_date['text'].head()\n",
    "df['col']=Twee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetwords = [Tweets_WithDate['text'][i].split() for i in range(len(Tweets_WithDate)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words.extend(['-', '&', 'â€¦', '', 'â€“', 'http://', ':', ',', ':'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordcount(tweets):\n",
    "    wordcount = {}\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            clean_word = word.strip('.' '!' '?' ')' '(' '#' ',' ':').lower()\n",
    "            clean_word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', clean_word, flags=re.MULTILINE)\n",
    "            \n",
    "            if clean_word not in stop_words:\n",
    "                if clean_word not in wordcount:\n",
    "                    count = tweet.count(word)\n",
    "                    wordcount[clean_word] = count\n",
    "                else:\n",
    "                    wordcount[clean_word] = wordcount[clean_word] + 1\n",
    "    return wordcount\n",
    "                \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordCount = wordcount(tweetwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxcount = [(word, wordCount[word]) for word in sorted(wordCount, key=wordCount.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tweets_WithDate['only_date'] = pd.DatetimeIndex(Tweets_WithDate['date']).date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_date=Tweets_WithDate.groupby(['only_date']).count()\n",
    "tweets_date['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tweets_WithDate['only_date'].sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tweets_date['text'].transpose().plot(kind='line',figsize=(6.5, 4))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('The number of demonitisation tweets per date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tweets_WithDate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
